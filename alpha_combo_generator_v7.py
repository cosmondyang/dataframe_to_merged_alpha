"""High-level pipeline to grow an equal-weight style alpha combo.

This module follows the specification outlined in the user request:

* Load a large feature parquet (generated by ``gen_all_date_feature.py``).
* Randomly sample composite features across the five feature families
  (price/time/volume/ratio+corr/count).
* Apply random unary / binary operator stacks (depth 1 – 4) in order to
  create an ``alpha`` candidate.
* Transform every ``alpha`` via six different distribution + scaling
  variants, test the combinations, and greedily add the variants that
  improve the global combo (``ic + lw_ic_v2`` grows by > 5e-4 while both
  components do not deteriorate).
* Keep an append-only TXT log that records every successful addition with a
  compact summary (combo metrics before/after plus the stochastic parameters),
  and continuously overwrite a parquet that stores the best combo series so far.
* Maintain an optional ``combo_generator_tmp`` cache of RNG seeds so new alphas
  can reuse pre-generated random plans without blocking the main loop.

The actual alpha search is stochastic and driven by the provided random
seed.  The implementation is intentionally modular: each of the individual
steps (pre-processing, composite generation, unary/binary operators,
distribution transforms, evaluation, logging) lives in its own helper class
so the behaviour can be verified and customised later on.

The code is long, but the logic is split into self-contained sections:

``AlphaWorkspace``
    Wraps the ``(pred_date, code)`` MultiIndex dataframe and exposes
    utilities such as random feature sampling and caching of preprocessed
    base columns.

``OperatorLibrary``
    Implements every unary/binary/composite operator described in the spec.

``DistributionEngine``
    Generates the six post-processing variations (original / rank /
    gaussian) x (two demean + scaling schemes).

``TouchstoneMetricBook``
    Delegates the IC / ``lw_ic_v2`` computation to ``examine_featuretocsv3``'s
    ``analyze`` helper so we reuse the same workflow as the existing toolchain.

``AlphaComboManager``
    Maintains the running combo, evaluates new candidates, writes the log
    file, and flushes the best combo to parquet.

Command line usage
==================

```
python alpha_combo_generator_v3.py \
    --parquet-path /path/to/features.parquet \
    --iterations 200 \
    --log-path combo_progress.txt \
    --combo-parquet best_combo.parquet \
    --figure-path figures/best_combo.png
```

The script will run until the iteration budget is exhausted (or until the
user stops it).  Intermediate progress lives entirely in the log; the
parquet file always reflects the current best combo only.

Note
====

The original request mentions several domain-specific operators
(``factorneut``, ``vectorneut`` …).  We implement sensible approximations
with pure pandas/numpy operations and document the precise formulas inside
the helper functions.  These approximations are deterministic and
vectorised; the structure can be extended easily if more accurate versions
become available later.
"""

from __future__ import annotations

import argparse
import dataclasses
import json
import math
import os
import random
import statistics
import sys
import textwrap
from dataclasses import dataclass
from typing import Dict, Iterable, List, Optional, Sequence, Tuple

import numpy as np
import pandas as pd

import matplotlib.pyplot as plt

plt.switch_backend("agg")

from examine_featuretocsv3 import analyze, safe_get_dates


# ---------------------------------------------------------------------------
# constants & utility helpers
# ---------------------------------------------------------------------------


EPS = 1e-8
DEFAULT_PARQUET_PATH = "/remote-home/yyc/gen_ocall_features_demo_v02/saved_features/fiveclass30sfeature2.parquet"
DEFAULT_PLAN_BUFFER_PATH = "combo_generator_tmp.json"


def _ensure_multi_index(df: pd.DataFrame) -> pd.DataFrame:
    if isinstance(df.index, pd.MultiIndex) and set(df.index.names) == {
        "pred_date",
        "code",
    }:
        out = df.sort_index()
        if not out.index.is_unique:
            out = out[~out.index.duplicated(keep="first")]
        return out
    if {"pred_date", "code"}.issubset(df.columns):
        out = df.set_index(["pred_date", "code"]).sort_index()
        if not out.index.is_unique:
            out = out[~out.index.duplicated(keep="first")]
        return out
    raise ValueError(
        "DataFrame must either have a MultiIndex (pred_date, code) or explicit columns."
    )


def _safe_rank(series: pd.Series) -> pd.Series:
    return series.rank(method="average", pct=True)


def _cs_apply(series: pd.Series, func) -> pd.Series:
    return series.groupby(level="pred_date").transform(func)


def _demean(series: pd.Series) -> pd.Series:
    return series - _cs_apply(series, "mean")


def _align_series_pair(a: pd.Series, b: pd.Series) -> Tuple[pd.Series, pd.Series]:
    """Align two series on their union index without raising."""

    def _normalise_index(series: pd.Series) -> pd.Series:
        idx = series.index
        if isinstance(idx, pd.MultiIndex):
            if idx.nlevels >= 2:
                idx = idx.set_names(["pred_date", "code"] + list(idx.names[2:]))
                series.index = idx
                return series.sort_index()
        series = series.copy()
        if isinstance(idx, pd.Index):
            # Treat the existing index as pred_date and broadcast a dummy code.
            mi = pd.MultiIndex.from_product(
                [idx, ["__missing__"]], names=["pred_date", "code"]
            )
            series.index = mi
        else:
            empty_index = pd.MultiIndex(
                levels=[[], []], codes=[[], []], names=["pred_date", "code"]
            )
            series = pd.Series(dtype=float, index=empty_index)
        return series.sort_index()

    a = _normalise_index(a)
    b = _normalise_index(b)
    if a.index.equals(b.index):
        return a, b
    union = a.index.union(b.index)
    a_aligned = a.reindex(union, fill_value=0.0)
    b_aligned = b.reindex(union, fill_value=0.0)
    return a_aligned.sort_index(), b_aligned.sort_index()


def _normalize_long_short(series: pd.Series) -> pd.Series:
    def _normalise_block(block: pd.Series) -> pd.Series:
        pos = block[block > 0]
        neg = block[block < 0]
        if pos.empty:
            pos = pos.copy()
        else:
            pos = pos / pos.abs().sum()
        if neg.empty:
            neg = neg.copy()
        else:
            neg = neg / neg.abs().sum()
        return pd.concat([pos, neg]).reindex(block.index, fill_value=0.0)

    return series.groupby(level="pred_date", group_keys=False).apply(
        _normalise_block
    )


def _normalize_long_short_mean(series: pd.Series) -> pd.Series:
    def _normalise_block(block: pd.Series) -> pd.Series:
        pos = block[block > 0]
        neg = block[block < 0]
        if not pos.empty:
            pos = pos / pos.sum()
        if not neg.empty:
            neg_mean = neg.mean()
            if abs(neg_mean) < EPS:
                neg = neg.copy()
            else:
                neg = neg / abs(neg_mean)
        return pd.concat([pos, neg]).reindex(block.index, fill_value=0.0)

    return series.groupby(level="pred_date", group_keys=False).apply(
        _normalise_block
    )


def _rolling_group_apply(series: pd.Series, window: int, func) -> pd.Series:
    results = []
    for code, block in series.groupby(level="code"):
        values = block.droplevel("code")
        rolled = values.rolling(window=window, min_periods=1).apply(func, raw=False)
        rolled.index = pd.MultiIndex.from_product(
            [rolled.index, [code]], names=["pred_date", "code"]
        )
        results.append(rolled)
    if not results:
        empty_index = pd.MultiIndex(levels=[[], []], codes=[[], []], names=["pred_date", "code"])
        return pd.Series(dtype=float, index=empty_index)
    return pd.concat(results).sort_index()


def _rolling_pair_apply(
    a: pd.Series, b: pd.Series, window: int, func
) -> pd.Series:
    a, b = _align_series_pair(a, b)

    def _apply(block_a: pd.Series, block_b: pd.Series) -> pd.Series:
        frame = pd.DataFrame({"a": block_a, "b": block_b})
        rolled = frame.rolling(window=window, min_periods=1).apply(
            lambda arr: func(arr[:, 0], arr[:, 1]), raw=True
        )
        return rolled["a"]

    if a.empty and b.empty:
        empty_index = pd.MultiIndex(
            levels=[[], []], codes=[[], []], names=["pred_date", "code"]
        )
        return pd.Series(dtype=float, index=empty_index)

    results = []
    codes = a.index.get_level_values("code").unique()
    for code in codes:
        try:
            block_a = a.xs(code, level="code")
        except KeyError:
            continue
        try:
            block_b = b.xs(code, level="code")
        except KeyError:
            block_b = pd.Series(0.0, index=block_a.index)
        block_b = block_b.reindex(block_a.index, fill_value=0.0)
        res = _apply(block_a, block_b)
        res.index = pd.MultiIndex.from_product(
            [block_a.index, [code]], names=["pred_date", "code"]
        )
        results.append(res)

    if not results:
        return pd.Series(0.0, index=a.index, dtype=float)

    combined = pd.concat(results).sort_index()
    return combined.reindex(a.index, fill_value=0.0)


def _gaussianize(series: pd.Series) -> pd.Series:
    ranks = _safe_rank(series)
    u = np.clip(ranks.values, EPS, 1 - EPS)
    # Use Abramowitz & Stegun approximation for erfinv when scipy is absent.
    def _approx_erfinv(x: np.ndarray) -> np.ndarray:
        a = 0.147  # good trade-off between precision & simplicity
        sign = np.sign(x)
        ln = np.log(1 - x ** 2)
        part = (2 / (math.pi * a)) + (ln / 2)
        inside = part ** 2 - ln / a
        return sign * np.sqrt(np.sqrt(inside) - part)

    gaussian = math.sqrt(2.0) * _approx_erfinv(2 * u - 1)
    return pd.Series(gaussian, index=series.index)


# ---------------------------------------------------------------------------
# Data workspace & preprocessing cache
# ---------------------------------------------------------------------------


@dataclass
class PreprocessRecord:
    column: str
    strategy: str


class AlphaWorkspace:
    """Manage feature sampling and preprocessing."""

    FILL_STRATEGIES = (
        "fill_zero",
        "fill_cs_mean",
        "fill_cs_max",
        "fill_cs_min",
        "fill_forward",
        "fill_rolling_mean20",
    )

    def __init__(self, df: pd.DataFrame, rng: Optional[random.Random] = None):
        self.df = _ensure_multi_index(df)
        self.df = self.df.sort_index()
        self.base_index = self.df.index
        self.categories = self._build_categories()
        self.cache: Dict[Tuple[str, str], pd.Series] = {}
        self.rng = rng or random.Random()

    def set_rng(self, rng: random.Random) -> None:
        self.rng = rng

    def _finalize_series(self, series: pd.Series) -> pd.Series:
        if not isinstance(series.index, pd.MultiIndex):
            raise ValueError("Preprocessed series must retain a (pred_date, code) MultiIndex.")
        series.index = series.index.set_names(["pred_date", "code"])
        series = series.reindex(self.base_index)
        series = series.fillna(0.0)
        return series.sort_index()

    def _build_categories(self) -> Dict[str, List[str]]:
        families = {
            "price": [],
            "time": [],
            "volume": [],
            "ratio": [],
            "corr": [],
            "count": [],
        }
        for col in self.df.columns:
            prefix = col.split("_")[0].lower()
            if prefix in families:
                families[prefix].append(col)
            elif prefix.startswith("ratio"):
                families["ratio"].append(col)
            elif prefix.startswith("corr") or prefix.startswith("cancel"):
                families["corr"].append(col)
            else:
                # treat unknown prefix as its own family
                families.setdefault(prefix, []).append(col)
        # merge ratio/corr into a single bucket as per spec
        families["ratio_corr"] = families.pop("ratio", []) + families.pop(
            "corr", []
        )
        return {k: v for k, v in families.items() if v}

    def random_family(self, exclude: Optional[str] = None) -> str:
        keys = [k for k in self.categories if k != exclude]
        if not keys:
            raise RuntimeError("No feature families available for sampling.")
        return self.rng.choice(keys)

    def random_feature(self, family: str) -> Tuple[pd.Series, PreprocessRecord]:
        col = self.rng.choice(self.categories[family])
        strategy = self.rng.choice(self.FILL_STRATEGIES)
        key = (col, strategy)
        if key not in self.cache:
            self.cache[key] = self._apply_preprocess(col, strategy)
        return self.cache[key], PreprocessRecord(column=col, strategy=strategy)

    # ------------------------------------------------------------------
    # preprocessing strategies
    # ------------------------------------------------------------------

    def _apply_preprocess(self, col: str, strategy: str) -> pd.Series:
        data = self.df[col].astype(float)
        if strategy == "fill_zero":
            return self._finalize_series(data.fillna(0.0))

        if strategy == "fill_cs_mean":
            filled = _cs_apply(data, "mean")
        elif strategy == "fill_cs_max":
            filled = _cs_apply(data, "max")
        elif strategy == "fill_cs_min":
            filled = _cs_apply(data, "min")
        elif strategy == "fill_forward":
            filled = (
                data.unstack("code")
                .sort_index()
                .ffill()
                .stack()
            )
        elif strategy == "fill_rolling_mean20":
            filled = _rolling_group_apply(data, 20, np.nanmean)
        else:
            raise ValueError(f"Unknown strategy: {strategy}")

        filled = filled.fillna(0.0)
        return self._finalize_series(filled)


# ---------------------------------------------------------------------------
# operator library
# ---------------------------------------------------------------------------


@dataclass
class CompositeStep:
    description: str
    families: Tuple[str, str]
    sources: Tuple[PreprocessRecord, PreprocessRecord]
    operator: str


@dataclass
class UnaryStep:
    name: str
    params: Dict[str, float]


@dataclass
class BinaryStep:
    name: str
    params: Dict[str, float]


@dataclass
class AlphaStructure:
    depth: int
    composite_steps: List[CompositeStep]
    unary_steps: List[UnaryStep]
    binary_steps: List[BinaryStep]


class OperatorLibrary:
    """Implements the composite/unary/binary operator zoo."""

    SAME_FAMILY_OPS = (
        "add",
        "divide",
        "multiply",
        "ratio",
        "abs_ratio",
        "max",
        "min",
        "zscore_sum",
        "demean_norm_sum",
    )

    CROSS_FAMILY_OPS = ("multiply", "zscore_sum", "demean_norm_sum")

    UNARY_OPS = (
        "ts_mean",
        "ts_zscore",
        "ts_max",
        "ts_min",
        "ts_regression",
        "ts_autocorr",
        "factor_neutralize",
        "cs_bucket",
        "cs_zscore",
        "cs_rank",
        "vector_neutralize_a_on_rank",
        "vector_neutralize_rank_on_a",
        "cs_regression_resid",
        "ts_rank_scale",
    )

    BINARY_OPS = (
        "rank_ratio",
        "abs_rank_ratio",
        "ts_regression_pair",
        "ts_corr",
        "product",
        "factor_neutralize_pair",
        "cs_regression_resid_pair",
        "bucket_transform",
        "centered_rank_product",
        "ts_zscore_product",
        "ts_cov",
        "rolling_rank_gap",
    )

    def __init__(self, workspace: AlphaWorkspace, rng: Optional[random.Random] = None):
        self.ws = workspace
        self.rng = rng or random.Random()

    def set_rng(self, rng: random.Random) -> None:
        self.rng = rng

    def _zero_series(self) -> pd.Series:
        return pd.Series(0.0, index=self.ws.base_index, dtype=float)

    def _finalize(self, series: Optional[pd.Series]) -> pd.Series:
        if series is None:
            return self._zero_series()
        try:
            series = series.astype(float)
        except Exception:
            return self._zero_series()
        try:
            if isinstance(series.index, pd.MultiIndex):
                series.index = series.index.set_names(["pred_date", "code"])
                series = series.sort_index()
            series = series.reindex(self.ws.base_index, fill_value=0.0)
        except Exception:
            return self._zero_series()
        return series.fillna(0.0)

    # ------------------------------------------------------------------
    # composite data
    # ------------------------------------------------------------------

    def random_composite(self) -> Tuple[pd.Series, CompositeStep]:
        same_family = self.rng.random() < 0.5
        if same_family:
            family = self.ws.random_family()
            op = self.rng.choice(self.SAME_FAMILY_OPS)
            series_a, rec_a = self.ws.random_feature(family)
            series_b, rec_b = self.ws.random_feature(family)
            try:
                comp = self._apply_same_family(series_a, series_b, op)
            except Exception:
                comp = self._zero_series()
            comp = self._finalize(comp)
            step = CompositeStep(
                description=f"same({family})",
                families=(family, family),
                sources=(rec_a, rec_b),
                operator=op,
            )
            return comp, step
        else:
            fam_a = self.ws.random_family()
            fam_b = self.ws.random_family(exclude=fam_a)
            op = self.rng.choice(self.CROSS_FAMILY_OPS)
            series_a, rec_a = self.ws.random_feature(fam_a)
            series_b, rec_b = self.ws.random_feature(fam_b)
            try:
                comp = self._apply_cross_family(series_a, series_b, op)
            except Exception:
                comp = self._zero_series()
            comp = self._finalize(comp)
            step = CompositeStep(
                description=f"cross({fam_a},{fam_b})",
                families=(fam_a, fam_b),
                sources=(rec_a, rec_b),
                operator=op,
            )
            return comp, step

    def _apply_same_family(
        self, a: pd.Series, b: pd.Series, op: str
    ) -> pd.Series:
        a, b = _align_series_pair(a, b)
        if op == "add":
            return a + b
        if op == "divide":
            return a / (b + EPS)
        if op == "multiply":
            return a * b
        if op == "ratio":
            return (a - b) / (a + b + EPS)
        if op == "abs_ratio":
            return (a - b).abs() / (a + b + EPS)
        if op == "max":
            return pd.concat([a, b], axis=1).max(axis=1)
        if op == "min":
            return pd.concat([a, b], axis=1).min(axis=1)
        if op == "zscore_sum":
            return _demean(a) / (_cs_apply(a, "std") + EPS) + _demean(b) / (
                _cs_apply(b, "std") + EPS
            )
        if op == "demean_norm_sum":
            normed_a = _normalize_long_short(_demean(a))
            normed_b = _normalize_long_short(_demean(b))
            return normed_a + normed_b
        raise ValueError(op)

    def _apply_cross_family(
        self, a: pd.Series, b: pd.Series, op: str
    ) -> pd.Series:
        a, b = _align_series_pair(a, b)
        if op == "multiply":
            return a * b
        if op == "zscore_sum":
            return _demean(a) / (_cs_apply(a, "std") + EPS) + _demean(b) / (
                _cs_apply(b, "std") + EPS
            )
        if op == "demean_norm_sum":
            normed_a = _normalize_long_short(_demean(a))
            normed_b = _normalize_long_short(_demean(b))
            return normed_a + normed_b
        raise ValueError(op)

    # ------------------------------------------------------------------
    # unary operators
    # ------------------------------------------------------------------

    def apply_unary(self, series: pd.Series, op: str) -> Tuple[pd.Series, UnaryStep]:
        if op == "ts_mean":
            window = self.rng.randint(2, 5)
            try:
                res = _rolling_group_apply(series, window, np.nanmean)
            except Exception:
                res = self._zero_series()
            return self._finalize(res), UnaryStep(name=op, params={"window": window})
        if op == "ts_zscore":
            window = self.rng.randint(2, 5)
            try:
                mean = _rolling_group_apply(series, window, np.nanmean)
                std = _rolling_group_apply(series, window, np.nanstd).replace(0, np.nan)
                res = (series - mean) / (std + EPS)
            except Exception:
                res = self._zero_series()
            return self._finalize(res), UnaryStep(name=op, params={"window": window})
        if op == "ts_max":
            try:
                res = _rolling_group_apply(series, 5, np.nanmax)
            except Exception:
                res = self._zero_series()
            return self._finalize(res), UnaryStep(name=op, params={"window": 5})
        if op == "ts_min":
            try:
                res = _rolling_group_apply(series, 5, np.nanmin)
            except Exception:
                res = self._zero_series()
            return self._finalize(res), UnaryStep(name=op, params={"window": 5})
        if op == "ts_regression":
            window = 5
            try:
                delay = series.groupby(level="code").shift(1)
                slope = self._rolling_regression(series, delay, window, "slope")
                choice = self.rng.choice(["slope", "intercept", "rmse"])
                if choice == "slope":
                    res = slope
                elif choice == "intercept":
                    intercept = self._rolling_regression(series, delay, window, "intercept")
                    res = intercept
                else:
                    res = self._rolling_regression(series, delay, window, "rmse")
            except Exception:
                choice = "slope"
                res = self._zero_series()
            return self._finalize(res), UnaryStep(name=f"{op}_{choice}", params={"window": window})
        if op == "ts_autocorr":
            try:
                res = self._rolling_autocorr(series, lag=1, window=5)
            except Exception:
                res = self._zero_series()
            return self._finalize(res), UnaryStep(name=op, params={"window": 5, "lag": 1})
        if op == "factor_neutralize":
            try:
                delay = series.groupby(level="code").shift(1)
                res = self._cross_sectional_regression(series, delay)["resid"]
            except Exception:
                res = self._zero_series()
            return self._finalize(res), UnaryStep(name=op, params={"factor": "delay1"})
        if op == "cs_bucket":
            buckets = self.rng.choice([5, 10])
            mode = self.rng.choice(["zscore", "rank", "mean"])
            try:
                res = self._apply_bucket(series, buckets, mode)
            except Exception:
                res = self._zero_series()
            return self._finalize(res), UnaryStep(
                name=f"{op}_{mode}", params={"buckets": buckets}
            )
        if op == "cs_zscore":
            try:
                res = _demean(series) / (_cs_apply(series, "std") + EPS)
            except Exception:
                res = self._zero_series()
            return self._finalize(res), UnaryStep(name=op, params={})
        if op == "cs_rank":
            try:
                res = series.groupby(level="pred_date").transform(_safe_rank)
            except Exception:
                res = self._zero_series()
            return self._finalize(res), UnaryStep(name=op, params={})
        if op == "vector_neutralize_a_on_rank":
            try:
                ranks = series.groupby(level="pred_date").transform(_safe_rank)
                res = self._cross_sectional_regression(series, ranks)["resid"]
            except Exception:
                res = self._zero_series()
            return self._finalize(res), UnaryStep(name=op, params={})
        if op == "vector_neutralize_rank_on_a":
            try:
                ranks = series.groupby(level="pred_date").transform(_safe_rank)
                res = self._cross_sectional_regression(ranks, series)["resid"]
            except Exception:
                res = self._zero_series()
            return self._finalize(res), UnaryStep(name=op, params={})
        if op == "cs_regression_resid":
            try:
                ranks = series.groupby(level="pred_date").transform(_safe_rank)
                res = self._cross_sectional_regression(series, ranks)["resid"]
            except Exception:
                res = self._zero_series()
            return self._finalize(res), UnaryStep(name=op, params={})
        if op == "ts_rank_scale":
            window = self.rng.randint(3, 6)
            try:
                res = (
                    series.groupby(level="code")
                    .rolling(window=window, min_periods=1)
                    .apply(lambda x: statistics.mean(_safe_rank(pd.Series(x))), raw=False)
                    .droplevel(0)
                )
            except Exception:
                res = self._zero_series()
            return self._finalize(res), UnaryStep(name=op, params={"window": window})
        raise ValueError(op)

    # ------------------------------------------------------------------
    # binary operators
    # ------------------------------------------------------------------

    def apply_binary(
        self, lhs: pd.Series, rhs: pd.Series, op: str
    ) -> Tuple[pd.Series, BinaryStep]:
        lhs, rhs = _align_series_pair(lhs, rhs)
        if op == "rank_ratio":
            try:
                rl = lhs.groupby(level="pred_date").transform(_safe_rank)
                rr = rhs.groupby(level="pred_date").transform(_safe_rank)
                res = (rl - rr) / (rl + rr + EPS)
            except Exception:
                res = self._zero_series()
            return self._finalize(res), BinaryStep(name=op, params={})
        if op == "abs_rank_ratio":
            try:
                rl = lhs.groupby(level="pred_date").transform(_safe_rank)
                rr = rhs.groupby(level="pred_date").transform(_safe_rank)
                res = (rl - rr).abs() / (rl + rr + EPS)
            except Exception:
                res = self._zero_series()
            return self._finalize(res), BinaryStep(name=op, params={})
        if op == "ts_regression_pair":
            window = 5
            try:
                res = self._rolling_regression(
                    lhs, rhs, window, self.rng.choice(["slope", "intercept", "rmse"])
                )
            except Exception:
                res = self._zero_series()
            return self._finalize(res), BinaryStep(name=op, params={"window": window})
        if op == "ts_corr":
            try:
                res = self._rolling_corr(lhs, rhs, window=5)
            except Exception:
                res = self._zero_series()
            return self._finalize(res), BinaryStep(name=op, params={"window": 5})
        if op == "product":
            try:
                res = lhs * rhs
            except Exception:
                res = self._zero_series()
            return self._finalize(res), BinaryStep(name=op, params={})
        if op == "factor_neutralize_pair":
            try:
                res = self._cross_sectional_regression(lhs, rhs)["resid"]
            except Exception:
                res = self._zero_series()
            return self._finalize(res), BinaryStep(name=op, params={})
        if op == "cs_regression_resid_pair":
            try:
                res = self._cross_sectional_regression(lhs, rhs)["resid"]
            except Exception:
                res = self._zero_series()
            return self._finalize(res), BinaryStep(name=op, params={})
        if op == "bucket_transform":
            buckets = self.rng.choice([5, 10])
            mode = self.rng.choice(["rank", "zscore", "mean"])
            try:
                grouped = rhs.groupby(level="pred_date").transform(
                    lambda s: pd.qcut(
                        s.rank(method="first"),
                        buckets,
                        labels=False,
                        duplicates="drop",
                    )
                )
                res = (
                    pd.DataFrame({"group": grouped, "lhs": lhs})
                    .groupby(level="pred_date", group_keys=False)
                    .apply(
                        lambda df: df.groupby("group")["lhs"].transform(
                            mode if mode != "mean" else "mean"
                        )
                    )
                )
                res.index = lhs.index
                res = res.sort_index()
            except Exception:
                res = self._zero_series()
            return self._finalize(res), BinaryStep(name=f"{op}_{mode}", params={"buckets": buckets})
        if op == "centered_rank_product":
            try:
                rl = lhs.groupby(level="pred_date").transform(_safe_rank) - 0.5
                rr = rhs.groupby(level="pred_date").transform(_safe_rank) - 0.5
                res = rl * rr
            except Exception:
                res = self._zero_series()
            return self._finalize(res), BinaryStep(name=op, params={})
        if op == "ts_zscore_product":
            try:
                zl = self.apply_unary(lhs, "ts_zscore")[0]
                zr = self.apply_unary(rhs, "ts_zscore")[0]
                res = zl * zr
            except Exception:
                res = self._zero_series()
            return self._finalize(res), BinaryStep(name=op, params={"window": "2-5"})
        if op == "ts_cov":
            try:
                res = self._rolling_cov(lhs, rhs, window=5)
            except Exception:
                res = self._zero_series()
            return self._finalize(res), BinaryStep(name=op, params={"window": 5})
        if op == "rolling_rank_gap":
            try:
                res = self._rolling_rank_gap(lhs, rhs, window=5)
            except Exception:
                res = self._zero_series()
            return self._finalize(res), BinaryStep(name=op, params={"window": 5})
        raise ValueError(op)

    # ------------------------------------------------------------------
    # helper calculations
    # ------------------------------------------------------------------

    def _rolling_regression(
        self, y: pd.Series, x: pd.Series, window: int, which: str
    ) -> pd.Series:
        def _calc(block_y: np.ndarray, block_x: np.ndarray) -> float:
            x_ = block_x
            y_ = block_y
            if np.all(np.isnan(x_)) or np.all(np.isnan(y_)):
                return np.nan
            mask = ~np.isnan(x_) & ~np.isnan(y_)
            if mask.sum() < 2:
                return np.nan
            x_ = x_[mask]
            y_ = y_[mask]
            x_mean = x_.mean()
            y_mean = y_.mean()
            cov = ((x_ - x_mean) * (y_ - y_mean)).sum()
            var = ((x_ - x_mean) ** 2).sum()
            slope = cov / (var + EPS)
            intercept = y_mean - slope * x_mean
            resid = y_ - (slope * x_ + intercept)
            if which == "slope":
                return slope
            if which == "intercept":
                return intercept
            rmse = np.sqrt(np.mean(resid ** 2))
            return rmse

        return _rolling_pair_apply(y, x, window, _calc)

    def _rolling_autocorr(self, series: pd.Series, lag: int, window: int) -> pd.Series:
        shifted = series.groupby(level="code").shift(lag)
        return self._rolling_corr(series, shifted, window)

    def _rolling_corr(
        self, a: pd.Series, b: pd.Series, window: int
    ) -> pd.Series:
        def _corr(x: np.ndarray, y: np.ndarray) -> float:
            mask = ~np.isnan(x) & ~np.isnan(y)
            if mask.sum() < 2:
                return np.nan
            return np.corrcoef(x[mask], y[mask])[0, 1]

        return _rolling_pair_apply(a, b, window, _corr)

    def _rolling_cov(
        self, a: pd.Series, b: pd.Series, window: int
    ) -> pd.Series:
        def _cov(x: np.ndarray, y: np.ndarray) -> float:
            mask = ~np.isnan(x) & ~np.isnan(y)
            if mask.sum() < 2:
                return np.nan
            return np.cov(x[mask], y[mask])[0, 1]

        return _rolling_pair_apply(a, b, window, _cov)

    def _rolling_rank_gap(
        self, a: pd.Series, b: pd.Series, window: int
    ) -> pd.Series:
        def _gap(x: np.ndarray, y: np.ndarray) -> float:
            mask = ~np.isnan(x) & ~np.isnan(y)
            if mask.sum() == 0:
                return np.nan
            xr = pd.Series(x[mask]).rank().values
            yr = pd.Series(y[mask]).rank().values
            return np.sum(np.abs(np.sort(xr) - np.sort(yr)))

        return _rolling_pair_apply(a, b, window, _gap)

    def _cross_sectional_regression(
        self, y: pd.Series, x: pd.Series
    ) -> Dict[str, pd.Series]:
        def _fit(block: pd.DataFrame) -> pd.DataFrame:
            df = block.dropna()
            if df.shape[0] < 2:
                return pd.DataFrame(
                    {
                        "slope": np.nan,
                        "intercept": np.nan,
                        "resid": np.nan,
                    },
                    index=block.index,
                )

            x_vals = df["x"].astype(float).to_numpy()
            y_vals = df["y"].astype(float).to_numpy()
            x_mean = float(np.mean(x_vals))
            y_mean = float(np.mean(y_vals))
            var = float(np.sum((x_vals - x_mean) ** 2))
            if not np.isfinite(var) or var <= EPS:
                slope = 0.0
            else:
                cov = float(np.sum((x_vals - x_mean) * (y_vals - y_mean)))
                slope = cov / (var + EPS)
            intercept = y_mean - slope * x_mean
            resid_vals = y_vals - (slope * x_vals + intercept)
            out = pd.DataFrame(
                {
                    "slope": np.full(df.shape[0], slope),
                    "intercept": np.full(df.shape[0], intercept),
                    "resid": resid_vals,
                },
                index=df.index,
            )
            return out.reindex(block.index)

        joined = pd.concat([y.rename("y"), x.rename("x")], axis=1)
        res = joined.groupby(level="pred_date", group_keys=False).apply(_fit)
        res.index = joined.index
        return {
            "slope": res["slope"],
            "intercept": res["intercept"],
            "resid": res["resid"],
        }

    def _apply_bucket(self, series: pd.Series, buckets: int, mode: str) -> pd.Series:
        def _transform(block: pd.Series) -> pd.Series:
            ranks = block.rank(method="first")
            labels = pd.qcut(
                ranks,
                q=min(buckets, block.notna().sum()),
                labels=False,
                duplicates="drop",
            )
            df = pd.DataFrame({"value": block, "label": labels})
            if mode == "zscore":
                agg = df.groupby("label")["value"].transform(lambda s: (s - s.mean()) / (s.std() + EPS))
            elif mode == "rank":
                agg = df.groupby("label")["value"].transform(lambda s: s.rank(pct=True))
            else:
                agg = df.groupby("label")["value"].transform("mean")
            return agg

        res = series.groupby(level="pred_date", group_keys=False).apply(_transform)
        res.index = series.index
        return res.sort_index()


# ---------------------------------------------------------------------------
# Distribution engine
# ---------------------------------------------------------------------------


class DistributionEngine:
    """Create the 6 (distribution x scaling) variants."""

    DISTRIBUTIONS = ("raw", "rank", "gaussian")
    SCALERS = ("norm1", "norm_mean")

    def __call__(self, series: pd.Series) -> Dict[str, pd.Series]:
        out: Dict[str, pd.Series] = {}
        for dist in self.DISTRIBUTIONS:
            if dist == "raw":
                transformed = series
            elif dist == "rank":
                transformed = series.groupby(level="pred_date").transform(_safe_rank)
            else:
                transformed = series.groupby(level="pred_date").transform(_gaussianize)
            demeaned = _demean(transformed)
            for scaler in self.SCALERS:
                if scaler == "norm1":
                    scaled = _normalize_long_short(demeaned)
                else:
                    scaled = _normalize_long_short_mean(demeaned)
                out[f"{dist}_{scaler}"] = scaled
        return out


# ---------------------------------------------------------------------------
# Metrics
# ---------------------------------------------------------------------------


@dataclass
class MetricResult:
    ic: float
    lw_ic_v2: float
    raw: Optional[Dict[str, object]] = None


class TouchstoneMetricBook:
    def __init__(self, state: str = "core", T: int = 20):
        self.state = state
        self.T = T

    def evaluate(self, series: pd.Series, label: str) -> MetricResult:
        factor_df = series.to_frame(name=label)
        try:
            res = analyze(factor_df, state=self.state, T=self.T)
        except Exception as exc:  # pragma: no cover - external service errors
            print(f"[TouchstoneMetricBook] analyze failed for {label}: {exc}")
            return MetricResult(ic=np.nan, lw_ic_v2=np.nan)

        ic_vals = np.asarray(res.get("ic", []), dtype="float64")
        lw_vals = np.asarray(res.get("lw_ic_v2", []), dtype="float64")

        ic = float(np.nanmean(ic_vals)) if ic_vals.size else float("nan")
        lw_ic = float(np.nanmean(lw_vals)) if lw_vals.size else float("nan")

        return MetricResult(ic=ic, lw_ic_v2=lw_ic, raw=res)


# ---------------------------------------------------------------------------
# Combo manager
# ---------------------------------------------------------------------------


@dataclass
class ComboEntry:
    name: str
    weight: float
    description: Dict[str, object]


class AlphaComboManager:
    def __init__(
        self,
        metric_book: TouchstoneMetricBook,
        log_path: str,
        combo_parquet: str,
        figure_path: str,
    ):
        self.metric_book = metric_book
        self.log_path = log_path
        self.combo_parquet = combo_parquet
        self.figure_path = figure_path
        self.entries: List[ComboEntry] = []
        self.combo_series: Optional[pd.Series] = None
        self.combo_metrics = MetricResult(ic=0.0, lw_ic_v2=0.0)
        self.best_combo_series: Optional[pd.Series] = None
        self.best_metric_sum = -np.inf
        self.combo_raw: Optional[Dict[str, object]] = None
        if os.path.exists(self.log_path):
            with open(self.log_path, "a", encoding="utf-8") as fp:
                fp.write("\n---- Restarting session ----\n")

    @staticmethod
    def _is_valid_metric(metrics: MetricResult) -> bool:
        return bool(
            metrics
            and np.isfinite(metrics.ic)
            and np.isfinite(metrics.lw_ic_v2)
        )

    def attempt_add(
        self,
        candidate_name: str,
        variants: Dict[str, pd.Series],
        structure: AlphaStructure,
        base_metrics: Dict[str, MetricResult],
    ) -> None:
        structure_dict = self._structure_to_dict(structure)
        for variant_key, variant_series in variants.items():
            variant_series = variant_series.sort_index()
            metrics = base_metrics.get(variant_key)
            if metrics is None:
                metrics = self.metric_book.evaluate(
                    variant_series, label=f"{candidate_name}_{variant_key}"
                )
                base_metrics[variant_key] = metrics
            if not self._is_valid_metric(metrics):
                continue
            working_series = variant_series
            working_metrics = metrics
            flipped = False
            if working_metrics.ic < 0:
                working_series = (-variant_series).sort_index()
                working_metrics = self.metric_book.evaluate(
                    working_series, label=f"{candidate_name}_{variant_key}_flipped"
                )
                flipped = True
            if (
                not self._is_valid_metric(working_metrics)
                or abs(working_metrics.ic) < 0.004
            ):
                continue
            for weight in (2.0, 1.0, 0.5, 0.1):
                previous_metrics = self.combo_metrics
                combined_series = self._combine_series(working_series, weight)
                combined_metrics = self.metric_book.evaluate(
                    combined_series, label="combo"
                )
                if not self._is_valid_metric(combined_metrics):
                    continue
                if self._is_improvement(combined_metrics):
                    self.combo_series = combined_series
                    self.entries.append(
                        ComboEntry(
                            name=f"{candidate_name}:{variant_key}",
                            weight=weight if not flipped else -weight,
                            description=structure_dict,
                        )
                    )
                    self.combo_metrics = combined_metrics
                    self.combo_raw = combined_metrics.raw
                    self._log_success(
                        candidate_name,
                        variant_key,
                        weight if not flipped else -weight,
                        combined_metrics,
                        working_metrics,
                        flipped,
                        structure_dict,
                        previous_metrics,
                    )
                    self._persist_combo()
                    break

    def _combine_series(self, series: pd.Series, weight: float) -> pd.Series:
        scaled = series.sort_index() * weight
        if self.combo_series is None:
            combined = scaled
        else:
            combo = self.combo_series.sort_index()
            df = pd.concat(
                [combo.rename("combo"), scaled.rename("candidate")], axis=1
            )
            combined = df.sum(axis=1, min_count=1)
        combined = combined.sort_index().rename("alpha_combo")
        combined = _normalize_long_short(_demean(combined.fillna(0.0)))
        combined.name = "alpha_combo"
        return combined.sort_index()

    def _is_improvement(self, metrics: MetricResult) -> bool:
        if pd.isna(metrics.ic) or pd.isna(metrics.lw_ic_v2):
            return False
        if metrics.ic < self.combo_metrics.ic - EPS:
            return False
        if metrics.lw_ic_v2 < self.combo_metrics.lw_ic_v2 - EPS:
            return False
        delta = (metrics.ic + metrics.lw_ic_v2) - (
            self.combo_metrics.ic + self.combo_metrics.lw_ic_v2
        )
        return delta > 5e-4

    def _metrics_snapshot(self, metrics: MetricResult) -> Dict[str, float]:
        return {
            "ic": float(metrics.ic) if metrics.ic is not None else float("nan"),
            "lw_ic_v2": float(metrics.lw_ic_v2)
            if metrics.lw_ic_v2 is not None
            else float("nan"),
        }

    def _log_success(
        self,
        candidate_name: str,
        variant_key: str,
        weight: float,
        new_metrics: MetricResult,
        variant_metrics: MetricResult,
        flipped: bool,
        structure: Dict[str, object],
        previous_metrics: MetricResult,
    ) -> None:
        os.makedirs(os.path.dirname(self.log_path) or ".", exist_ok=True)
        payload = {
            "combo_before": self._metrics_snapshot(previous_metrics),
            "combo_after": self._metrics_snapshot(new_metrics),
            "candidate": {
                "name": candidate_name,
                "variant": variant_key,
                "weight": weight,
                "flipped": flipped,
                "metrics": self._metrics_snapshot(variant_metrics),
            },
            "structure": structure,
        }
        with open(self.log_path, "a", encoding="utf-8") as fp:
            fp.write(json.dumps(payload, ensure_ascii=False) + "\n")
        print(
            (
                f"[combo] Added {candidate_name}:{variant_key} (weight {weight:+.2f})"
                f" | IC {previous_metrics.ic:.6f} -> {new_metrics.ic:.6f}"
                f" | LW_IC_V2 {previous_metrics.lw_ic_v2:.6f} -> {new_metrics.lw_ic_v2:.6f}"
            ),
            flush=True,
        )

    def _persist_combo(self) -> None:
        total = self.combo_metrics.ic + self.combo_metrics.lw_ic_v2
        if total <= self.best_metric_sum:
            return
        self.best_metric_sum = total
        self.best_combo_series = self.combo_series
        os.makedirs(os.path.dirname(self.combo_parquet) or ".", exist_ok=True)
        combo_df = self.combo_series.to_frame(name="alpha_combo")
        combo_df.to_parquet(self.combo_parquet)
        if self.combo_raw:
            _save_combo_plot(self.combo_raw, self.figure_path)

    def _structure_to_dict(self, structure: AlphaStructure) -> Dict[str, object]:
        return {
            "depth": structure.depth,
            "composites": [dataclasses.asdict(step) for step in structure.composite_steps],
            "unary": [dataclasses.asdict(step) for step in structure.unary_steps],
            "binary": [dataclasses.asdict(step) for step in structure.binary_steps],
        }


# ---------------------------------------------------------------------------
# plotting helpers
# ---------------------------------------------------------------------------


def _save_combo_plot(res_dict: Dict[str, object], figure_path: str) -> None:
    dates = safe_get_dates(res_dict)
    ic_vals = np.asarray(res_dict.get("ic", []), dtype="float64")
    lw_vals = np.asarray(res_dict.get("lw_ic_v2", []), dtype="float64")

    if not dates or ic_vals.size == 0 or lw_vals.size == 0:
        return

    os.makedirs(os.path.dirname(figure_path) or ".", exist_ok=True)

    fig, (ax0, ax1) = plt.subplots(2, 1, figsize=(12, 10), sharex=True)
    ax0.plot(dates, ic_vals, label="IC")
    ax0.plot(dates, lw_vals, label="LW_IC_V2")
    ax0.legend()
    ax0.set_title("Daily metrics for best combo")

    ax1.plot(dates, np.nancumsum(ic_vals), label="CumSum IC")
    ax1.plot(dates, np.nancumsum(lw_vals), label="CumSum LW_IC_V2")
    ax1.legend()
    ax1.set_title("Cumulative metrics")

    fig.tight_layout()
    fig.savefig(figure_path)
    plt.close(fig)


# ---------------------------------------------------------------------------
# Random plan buffer
# ---------------------------------------------------------------------------


class RandomPlanBuffer:
    """Persist and recycle per-alpha RNG seeds to amortize sampling cost."""

    def __init__(self, path: Optional[str], refill_size: int = 100):
        self.path = path if path else None
        self.refill_size = max(1, int(refill_size))
        self.queue: List[int] = []
        if self.path:
            self._load()

    # public API -----------------------------------------------------

    def enabled(self) -> bool:
        return self.path is not None

    def prime(self, rng: random.Random) -> None:
        if not self.enabled():
            return
        if not self.queue:
            self._refill(rng)

    def next_seed(self, rng: random.Random) -> int:
        if not self.enabled():
            return rng.getrandbits(63)
        if not self.queue:
            self._refill(rng)
        seed = int(self.queue.pop(0))
        if len(self.queue) < max(1, self.refill_size // 2):
            self._refill(rng)
        self._dump()
        print(
            f"[RandomPlanBuffer] Provided seed {seed} | remaining {len(self.queue)}",
            flush=True,
        )
        return seed

    # persistence helpers -------------------------------------------

    def _load(self) -> None:
        try:
            with open(self.path, "r", encoding="utf-8") as fh:
                payload = json.load(fh)
        except FileNotFoundError:
            self.queue = []
            return
        except Exception as exc:  # pragma: no cover - diagnostics only
            print(f"[RandomPlanBuffer] Failed to load {self.path}: {exc}")
            self.queue = []
            return
        data = payload.get("seeds") if isinstance(payload, dict) else payload
        if isinstance(data, list):
            self.queue = [int(x) for x in data]
        else:
            self.queue = []

    def _dump(self) -> None:
        if not self.enabled():
            return
        tmp_path = f"{self.path}.tmp"
        os.makedirs(os.path.dirname(self.path) or ".", exist_ok=True)
        with open(tmp_path, "w", encoding="utf-8") as fh:
            json.dump({"seeds": self.queue}, fh, indent=2)
        os.replace(tmp_path, self.path)

    def _refill(self, rng: random.Random) -> None:
        target = self.refill_size - len(self.queue)
        if target <= 0:
            return
        self.queue.extend(rng.getrandbits(63) for _ in range(target))
        self._dump()
        print(
            f"[RandomPlanBuffer] Refilled {target} seeds | total {len(self.queue)}",
            flush=True,
        )


# ---------------------------------------------------------------------------
# Alpha builder
# ---------------------------------------------------------------------------


class AlphaBuilder:
    def __init__(self, operator_lib: OperatorLibrary, rng: Optional[random.Random] = None):
        self.oplib = operator_lib
        self.rng = rng or random.Random()

    def set_rng(self, rng: random.Random) -> None:
        self.rng = rng

    def build(self) -> Tuple[pd.Series, AlphaStructure]:
        depth = self.rng.randint(1, 4)
        unary_used = False
        current: Optional[pd.Series] = None
        composite_steps: List[CompositeStep] = []
        unary_steps: List[UnaryStep] = []
        binary_steps: List[BinaryStep] = []

        for _ in range(depth):
            comp_series, comp_step = self.oplib.random_composite()
            composite_steps.append(comp_step)

            if current is None:
                current = comp_series
                continue

            use_unary = (not unary_used) and (self.rng.random() < 0.3)
            current, comp_series = _align_series_pair(current, comp_series)
            if use_unary:
                op = self.rng.choice(self.oplib.UNARY_OPS)
                current, unary_step = self.oplib.apply_unary(current, op)
                unary_steps.append(unary_step)
                unary_used = True
                current = current + comp_series
            else:
                op = self.rng.choice(self.oplib.BINARY_OPS)
                current, binary_step = self.oplib.apply_binary(current, comp_series, op)
                binary_steps.append(binary_step)

        if current is None:
            raise RuntimeError("Failed to build alpha – no composites generated.")

        current = current.sort_index()
        structure = AlphaStructure(
            depth=depth,
            composite_steps=composite_steps,
            unary_steps=unary_steps,
            binary_steps=binary_steps,
        )
        return current, structure


# ---------------------------------------------------------------------------
# High level driver
# ---------------------------------------------------------------------------


def run_search(
    df: pd.DataFrame,
    iterations: int,
    seed: Optional[int],
    log_path: str,
    combo_parquet: str,
    figure_path: str,
    touchstone_state: str,
    touchstone_T: int,
    plan_buffer_path: Optional[str],
    plan_buffer_refill: int,
):
    base_rng = random.Random(seed) if seed is not None else random.Random()
    if seed is not None:
        random.seed(seed)
        np.random.seed(seed)

    plan_path = None
    if plan_buffer_path:
        lowered = plan_buffer_path.strip().lower()
        if lowered not in {"none", ""}:
            plan_path = plan_buffer_path

    plan_buffer = RandomPlanBuffer(plan_path, refill_size=plan_buffer_refill)
    plan_buffer.prime(base_rng)

    workspace = AlphaWorkspace(df, rng=base_rng)
    operator_lib = OperatorLibrary(workspace, rng=base_rng)
    builder = AlphaBuilder(operator_lib, rng=base_rng)
    metric_book = TouchstoneMetricBook(state=touchstone_state, T=touchstone_T)
    combo_manager = AlphaComboManager(
        metric_book,
        log_path=log_path,
        combo_parquet=combo_parquet,
        figure_path=figure_path,
    )
    distribution_engine = DistributionEngine()

    for iteration in range(1, iterations + 1):
        local_seed = plan_buffer.next_seed(base_rng)
        local_rng = random.Random(local_seed)
        workspace.set_rng(local_rng)
        operator_lib.set_rng(local_rng)
        builder.set_rng(local_rng)
        series, structure = builder.build()
        candidate_name = f"alpha_{iteration:05d}"
        variants = distribution_engine(series)
        base_metrics: Dict[str, MetricResult] = {}
        combo_manager.attempt_add(candidate_name, variants, structure, base_metrics)


def _parse_args(argv: Optional[Sequence[str]] = None) -> argparse.Namespace:
    parser = argparse.ArgumentParser(
        description="Randomly expand dataframe features into a combo alpha",
    )
    parser.add_argument(
        "--parquet-path",
        default=DEFAULT_PARQUET_PATH,
        help=(
            "Path to the parquet file produced by gen_all_date_feature.py. "
            "Defaults to the shared demo file if not provided."
        ),
    )
    parser.add_argument(
        "--iterations",
        type=int,
        default=200,
        help="Number of stochastic iterations to run",
    )
    parser.add_argument(
        "--seed",
        type=int,
        default=None,
        help="Random seed for reproducibility",
    )
    parser.add_argument(
        "--log-path",
        default="combo_progress.txt",
        help="TXT file used to log successful alpha additions",
    )
    parser.add_argument(
        "--combo-parquet",
        default="best_combo.parquet",
        help="Parquet path that stores the strongest combo so far",
    )
    parser.add_argument(
        "--figure-path",
        default="figures/best_combo.png",
        help="PNG figure storing the metrics of the strongest combo",
    )
    parser.add_argument(
        "--plan-buffer",
        default=DEFAULT_PLAN_BUFFER_PATH,
        help=(
            "Path to the JSON cache that stores pre-generated RNG seeds. "
            "Pass 'none' to disable the cache."
        ),
    )
    parser.add_argument(
        "--plan-buffer-refill",
        type=int,
        default=100,
        help=(
            "How many seeds to keep buffered for future alphas when the "
            "plan cache is enabled."
        ),
    )
    parser.add_argument(
        "--touchstone-state",
        default="core",
        help="State argument forwarded to Touchstone analyze()",
    )
    parser.add_argument(
        "--touchstone-T",
        type=int,
        default=20,
        help="Window length forwarded to Touchstone analyze()",
    )
    return parser.parse_args(argv)


def main(argv: Optional[Sequence[str]] = None) -> None:
    args = _parse_args(argv)
    parquet_path = os.path.expanduser(args.parquet_path)
    if not os.path.exists(parquet_path):
        raise FileNotFoundError(
            f"Could not locate parquet file at '{parquet_path}'. "
            "Pass --parquet-path with the correct location."
        )
    df = pd.read_parquet(parquet_path)
    run_search(
        df=df,
        iterations=args.iterations,
        seed=args.seed,
        log_path=args.log_path,
        combo_parquet=args.combo_parquet,
        figure_path=args.figure_path,
        touchstone_state=args.touchstone_state,
        touchstone_T=args.touchstone_T,
        plan_buffer_path=args.plan_buffer,
        plan_buffer_refill=args.plan_buffer_refill,
    )


if __name__ == "__main__":
    main()

