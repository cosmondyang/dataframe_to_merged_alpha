"""High-level pipeline to grow an equal-weight style alpha combo.

This module follows the specification outlined in the user request:

* Load a large feature parquet (generated by ``gen_all_date_feature.py``).
* Randomly sample composite features across the five feature families
  (price/time/volume/ratio+cancel/count).
* Apply random unary / binary operator stacks (depth 1 – 4) in order to
  create an ``alpha`` candidate.
* Transform every ``alpha`` via six different distribution + scaling
  variants, test the combinations, and greedily add the variants that
  improve the global combo (``ic + lw_ic_v2`` grows by > 5e-4 while both
  components do not deteriorate).
* Keep an append-only TXT log that records every successful addition, and
  continuously overwrite a parquet that stores the best combo series so far.

The actual alpha search is stochastic and driven by the provided random
seed.  The implementation is intentionally modular: each of the individual
steps (pre-processing, composite generation, unary/binary operators,
distribution transforms, evaluation, logging) lives in its own helper class
so the behaviour can be verified and customised later on.

The code is long, but the logic is split into self-contained sections:

``AlphaWorkspace``
    Wraps the ``(pred_date, code)`` MultiIndex dataframe and exposes
    utilities such as random feature sampling and caching of preprocessed
    base columns.

``OperatorLibrary``
    Implements every unary/binary/composite operator described in the spec.

``DistributionEngine``
    Generates the six post-processing variations (original / rank /
    gaussian) x (two demean + scaling schemes).

``TouchstoneMetricBook``
    Delegates the IC / ``lw_ic_v2`` computation to ``examine_featuretocsv3``'s
    ``analyze`` helper so we reuse the same workflow as the existing toolchain.

``AlphaComboManager``
    Maintains the running combo, evaluates new candidates, writes the log
    file, and flushes the best combo to parquet.

Command line usage
==================

```
python alpha_combo_generator_v3.py \
    --parquet-path /path/to/features.parquet \
    --iterations 200 \
    --log-path combo_progress.txt \
    --combo-parquet best_combo.parquet \
    --figure-path figures/best_combo.png
```

The script will run until the iteration budget is exhausted (or until the
user stops it).  Intermediate progress lives entirely in the log; the
parquet file always reflects the current best combo only.

Note
====

The original request mentions several domain-specific operators
(``factorneut``, ``vectorneut`` …).  We implement sensible approximations
with pure pandas/numpy operations and document the precise formulas inside
the helper functions.  These approximations are deterministic and
vectorised; the structure can be extended easily if more accurate versions
become available later.
"""

from __future__ import annotations

import argparse
import dataclasses
import json
import math
import os
import random
import statistics
import sys
import textwrap
from dataclasses import dataclass
from typing import Dict, Iterable, List, Optional, Sequence, Tuple

import numpy as np
import pandas as pd

import matplotlib.pyplot as plt

plt.switch_backend("agg")

from examine_featuretocsv3 import analyze, safe_get_dates


# ---------------------------------------------------------------------------
# constants & utility helpers
# ---------------------------------------------------------------------------


EPS = 1e-8
DEFAULT_PARQUET_PATH = "/remote-home/yyc/gen_ocall_features_demo_v02/saved_features/fiveclass30sfeature2.parquet"


def _ensure_multi_index(df: pd.DataFrame) -> pd.DataFrame:
    if isinstance(df.index, pd.MultiIndex) and set(df.index.names) == {
        "pred_date",
        "code",
    }:
        out = df.sort_index()
        if not out.index.is_unique:
            out = out[~out.index.duplicated(keep="first")]
        return out
    if {"pred_date", "code"}.issubset(df.columns):
        out = df.set_index(["pred_date", "code"]).sort_index()
        if not out.index.is_unique:
            out = out[~out.index.duplicated(keep="first")]
        return out
    raise ValueError(
        "DataFrame must either have a MultiIndex (pred_date, code) or explicit columns."
    )


def _safe_rank(series: pd.Series) -> pd.Series:
    return series.rank(method="average", pct=True)


def _cs_apply(series: pd.Series, func) -> pd.Series:
    return series.groupby(level="pred_date").transform(func)


def _demean(series: pd.Series) -> pd.Series:
    return series - _cs_apply(series, "mean")


def _align_series_pair(a: pd.Series, b: pd.Series) -> Tuple[pd.Series, pd.Series]:
    a = a.sort_index()
    b = b.sort_index()
    if isinstance(a.index, pd.MultiIndex):
        a.index = a.index.set_names(["pred_date", "code"])
    if isinstance(b.index, pd.MultiIndex):
        b.index = b.index.set_names(["pred_date", "code"])
    if a.index.equals(b.index):
        return a, b
    common = a.index.intersection(b.index)
    if len(common) == 0:
        union = a.index.union(b.index)
        a_aligned = a.reindex(union, fill_value=0.0)
        b_aligned = b.reindex(union, fill_value=0.0)
        return a_aligned.sort_index(), b_aligned.sort_index()
    a_aligned = a.loc[common]
    b_aligned = b.loc[common]
    return a_aligned, b_aligned


def _normalize_long_short(series: pd.Series) -> pd.Series:
    def _normalise_block(block: pd.Series) -> pd.Series:
        pos = block[block > 0]
        neg = block[block < 0]
        if pos.empty:
            pos = pos.copy()
        else:
            pos = pos / pos.abs().sum()
        if neg.empty:
            neg = neg.copy()
        else:
            neg = neg / neg.abs().sum()
        return pd.concat([pos, neg]).reindex(block.index, fill_value=0.0)

    return series.groupby(level="pred_date", group_keys=False).apply(
        _normalise_block
    )


def _normalize_long_short_mean(series: pd.Series) -> pd.Series:
    def _normalise_block(block: pd.Series) -> pd.Series:
        pos = block[block > 0]
        neg = block[block < 0]
        if not pos.empty:
            pos = pos / pos.sum()
        if not neg.empty:
            neg_mean = neg.mean()
            if abs(neg_mean) < EPS:
                neg = neg.copy()
            else:
                neg = neg / abs(neg_mean)
        return pd.concat([pos, neg]).reindex(block.index, fill_value=0.0)

    return series.groupby(level="pred_date", group_keys=False).apply(
        _normalise_block
    )


def _rolling_group_apply(series: pd.Series, window: int, func) -> pd.Series:
    results = []
    for code, block in series.groupby(level="code"):
        values = block.droplevel("code")
        rolled = values.rolling(window=window, min_periods=1).apply(func, raw=False)
        rolled.index = pd.MultiIndex.from_product(
            [rolled.index, [code]], names=["pred_date", "code"]
        )
        results.append(rolled)
    if not results:
        empty_index = pd.MultiIndex(levels=[[], []], codes=[[], []], names=["pred_date", "code"])
        return pd.Series(dtype=float, index=empty_index)
    return pd.concat(results).sort_index()


def _rolling_pair_apply(
    a: pd.Series, b: pd.Series, window: int, func
) -> pd.Series:
    def _apply(block_a: pd.Series, block_b: pd.Series) -> pd.Series:
        return (
            pd.DataFrame({"a": block_a, "b": block_b})
            .rolling(window=window, min_periods=1)
            .apply(lambda arr: func(arr[:, 0], arr[:, 1]), raw=True)
        )

    results = []
    for code, sub_a in a.groupby(level="code"):
        sub_b = b.xs(code, level="code")
        res = _apply(sub_a.droplevel("code"), sub_b.droplevel("code"))
        res.index = pd.MultiIndex.from_product(
            [sub_a.index.get_level_values("pred_date"), [code]],
            names=["pred_date", "code"],
        )
        results.append(res["a"])
    return pd.concat(results).sort_index()


def _gaussianize(series: pd.Series) -> pd.Series:
    ranks = _safe_rank(series)
    u = np.clip(ranks.values, EPS, 1 - EPS)
    # Use Abramowitz & Stegun approximation for erfinv when scipy is absent.
    def _approx_erfinv(x: np.ndarray) -> np.ndarray:
        a = 0.147  # good trade-off between precision & simplicity
        sign = np.sign(x)
        ln = np.log(1 - x ** 2)
        part = (2 / (math.pi * a)) + (ln / 2)
        inside = part ** 2 - ln / a
        return sign * np.sqrt(np.sqrt(inside) - part)

    gaussian = math.sqrt(2.0) * _approx_erfinv(2 * u - 1)
    return pd.Series(gaussian, index=series.index)


# ---------------------------------------------------------------------------
# Data workspace & preprocessing cache
# ---------------------------------------------------------------------------


@dataclass
class PreprocessRecord:
    column: str
    strategy: str


class AlphaWorkspace:
    """Manage feature sampling and preprocessing."""

    FILL_STRATEGIES = (
        "fill_zero",
        "fill_cs_mean",
        "fill_cs_max",
        "fill_cs_min",
        "fill_forward",
        "fill_rolling_mean20",
    )

    def __init__(self, df: pd.DataFrame):
        self.df = _ensure_multi_index(df)
        self.df = self.df.sort_index()
        self.base_index = self.df.index
        self.categories = self._build_categories()
        self.cache: Dict[Tuple[str, str], pd.Series] = {}

    def _finalize_series(self, series: pd.Series) -> pd.Series:
        if not isinstance(series.index, pd.MultiIndex):
            raise ValueError("Preprocessed series must retain a (pred_date, code) MultiIndex.")
        series.index = series.index.set_names(["pred_date", "code"])
        series = series.reindex(self.base_index)
        series = series.fillna(0.0)
        return series.sort_index()

    def _build_categories(self) -> Dict[str, List[str]]:
        families = {
            "price": [],
            "time": [],
            "volume": [],
            "ratio": [],
            "cancel": [],
            "count": [],
        }
        for col in self.df.columns:
            prefix = col.split("_")[0].lower()
            if prefix in families:
                families[prefix].append(col)
            elif prefix.startswith("ratio"):
                families["ratio"].append(col)
            elif prefix.startswith("cancel"):
                families["cancel"].append(col)
            else:
                # treat unknown prefix as its own family
                families.setdefault(prefix, []).append(col)
        # merge ratio/cancel into a single bucket as per spec
        families["ratio_cancel"] = families.pop("ratio", []) + families.pop(
            "cancel", []
        )
        return {k: v for k, v in families.items() if v}

    def random_family(self, exclude: Optional[str] = None) -> str:
        keys = [k for k in self.categories if k != exclude]
        if not keys:
            raise RuntimeError("No feature families available for sampling.")
        return random.choice(keys)

    def random_feature(self, family: str) -> Tuple[pd.Series, PreprocessRecord]:
        col = random.choice(self.categories[family])
        strategy = random.choice(self.FILL_STRATEGIES)
        key = (col, strategy)
        if key not in self.cache:
            self.cache[key] = self._apply_preprocess(col, strategy)
        return self.cache[key], PreprocessRecord(column=col, strategy=strategy)

    # ------------------------------------------------------------------
    # preprocessing strategies
    # ------------------------------------------------------------------

    def _apply_preprocess(self, col: str, strategy: str) -> pd.Series:
        data = self.df[col].astype(float)
        if strategy == "fill_zero":
            return self._finalize_series(data.fillna(0.0))

        if strategy == "fill_cs_mean":
            filled = _cs_apply(data, "mean")
        elif strategy == "fill_cs_max":
            filled = _cs_apply(data, "max")
        elif strategy == "fill_cs_min":
            filled = _cs_apply(data, "min")
        elif strategy == "fill_forward":
            filled = (
                data.unstack("code")
                .sort_index()
                .ffill()
                .stack()
            )
        elif strategy == "fill_rolling_mean20":
            filled = _rolling_group_apply(data, 20, np.nanmean)
        else:
            raise ValueError(f"Unknown strategy: {strategy}")

        filled = filled.fillna(0.0)
        return self._finalize_series(filled)


# ---------------------------------------------------------------------------
# operator library
# ---------------------------------------------------------------------------


@dataclass
class CompositeStep:
    description: str
    families: Tuple[str, str]
    sources: Tuple[PreprocessRecord, PreprocessRecord]
    operator: str


@dataclass
class UnaryStep:
    name: str
    params: Dict[str, float]


@dataclass
class BinaryStep:
    name: str
    params: Dict[str, float]


@dataclass
class AlphaStructure:
    depth: int
    composite_steps: List[CompositeStep]
    unary_steps: List[UnaryStep]
    binary_steps: List[BinaryStep]


class OperatorLibrary:
    """Implements the composite/unary/binary operator zoo."""

    SAME_FAMILY_OPS = (
        "add",
        "divide",
        "multiply",
        "ratio",
        "abs_ratio",
        "max",
        "min",
        "zscore_sum",
        "demean_norm_sum",
    )

    CROSS_FAMILY_OPS = ("multiply", "zscore_sum", "demean_norm_sum")

    UNARY_OPS = (
        "ts_mean",
        "ts_zscore",
        "ts_max",
        "ts_min",
        "ts_regression",
        "ts_autocorr",
        "factor_neutralize",
        "cs_bucket",
        "cs_zscore",
        "cs_rank",
        "vector_neutralize_a_on_rank",
        "vector_neutralize_rank_on_a",
        "cs_regression_resid",
        "ts_rank_scale",
    )

    BINARY_OPS = (
        "rank_ratio",
        "abs_rank_ratio",
        "ts_regression_pair",
        "ts_corr",
        "product",
        "factor_neutralize_pair",
        "cs_regression_resid_pair",
        "bucket_transform",
        "centered_rank_product",
        "ts_zscore_product",
        "ts_cov",
        "rolling_rank_gap",
    )

    def __init__(self, workspace: AlphaWorkspace):
        self.ws = workspace

    # ------------------------------------------------------------------
    # composite data
    # ------------------------------------------------------------------

    def random_composite(self) -> Tuple[pd.Series, CompositeStep]:
        same_family = random.random() < 0.5
        if same_family:
            family = self.ws.random_family()
            op = random.choice(self.SAME_FAMILY_OPS)
            series_a, rec_a = self.ws.random_feature(family)
            series_b, rec_b = self.ws.random_feature(family)
            comp = self._apply_same_family(series_a, series_b, op)
            step = CompositeStep(
                description=f"same({family})",
                families=(family, family),
                sources=(rec_a, rec_b),
                operator=op,
            )
            return comp, step
        else:
            fam_a = self.ws.random_family()
            fam_b = self.ws.random_family(exclude=fam_a)
            op = random.choice(self.CROSS_FAMILY_OPS)
            series_a, rec_a = self.ws.random_feature(fam_a)
            series_b, rec_b = self.ws.random_feature(fam_b)
            comp = self._apply_cross_family(series_a, series_b, op)
            step = CompositeStep(
                description=f"cross({fam_a},{fam_b})",
                families=(fam_a, fam_b),
                sources=(rec_a, rec_b),
                operator=op,
            )
            return comp, step

    def _apply_same_family(
        self, a: pd.Series, b: pd.Series, op: str
    ) -> pd.Series:
        a, b = _align_series_pair(a, b)
        if op == "add":
            return a + b
        if op == "divide":
            return a / (b + EPS)
        if op == "multiply":
            return a * b
        if op == "ratio":
            return (a - b) / (a + b + EPS)
        if op == "abs_ratio":
            return (a - b).abs() / (a + b + EPS)
        if op == "max":
            return pd.concat([a, b], axis=1).max(axis=1)
        if op == "min":
            return pd.concat([a, b], axis=1).min(axis=1)
        if op == "zscore_sum":
            return _demean(a) / (_cs_apply(a, "std") + EPS) + _demean(b) / (
                _cs_apply(b, "std") + EPS
            )
        if op == "demean_norm_sum":
            normed_a = _normalize_long_short(_demean(a))
            normed_b = _normalize_long_short(_demean(b))
            return normed_a + normed_b
        raise ValueError(op)

    def _apply_cross_family(
        self, a: pd.Series, b: pd.Series, op: str
    ) -> pd.Series:
        a, b = _align_series_pair(a, b)
        if op == "multiply":
            return a * b
        if op == "zscore_sum":
            return _demean(a) / (_cs_apply(a, "std") + EPS) + _demean(b) / (
                _cs_apply(b, "std") + EPS
            )
        if op == "demean_norm_sum":
            normed_a = _normalize_long_short(_demean(a))
            normed_b = _normalize_long_short(_demean(b))
            return normed_a + normed_b
        raise ValueError(op)

    # ------------------------------------------------------------------
    # unary operators
    # ------------------------------------------------------------------

    def apply_unary(self, series: pd.Series, op: str) -> Tuple[pd.Series, UnaryStep]:
        if op == "ts_mean":
            window = random.randint(2, 5)
            res = _rolling_group_apply(series, window, np.nanmean)
            return res, UnaryStep(name=op, params={"window": window})
        if op == "ts_zscore":
            window = random.randint(2, 5)
            mean = _rolling_group_apply(series, window, np.nanmean)
            std = _rolling_group_apply(series, window, np.nanstd).replace(0, np.nan)
            res = (series - mean) / (std + EPS)
            return res, UnaryStep(name=op, params={"window": window})
        if op == "ts_max":
            res = _rolling_group_apply(series, 5, np.nanmax)
            return res, UnaryStep(name=op, params={"window": 5})
        if op == "ts_min":
            res = _rolling_group_apply(series, 5, np.nanmin)
            return res, UnaryStep(name=op, params={"window": 5})
        if op == "ts_regression":
            window = 5
            delay = series.groupby(level="code").shift(1)
            slope = self._rolling_regression(series, delay, window, "slope")
            choice = random.choice(["slope", "intercept", "rmse"])
            if choice == "slope":
                res = slope
            elif choice == "intercept":
                intercept = self._rolling_regression(series, delay, window, "intercept")
                res = intercept
            else:
                res = self._rolling_regression(series, delay, window, "rmse")
            return res, UnaryStep(name=f"{op}_{choice}", params={"window": window})
        if op == "ts_autocorr":
            res = self._rolling_autocorr(series, lag=1, window=5)
            return res, UnaryStep(name=op, params={"window": 5, "lag": 1})
        if op == "factor_neutralize":
            delay = series.groupby(level="code").shift(1)
            res = self._cross_sectional_regression(series, delay)["resid"]
            return res, UnaryStep(name=op, params={"factor": "delay1"})
        if op == "cs_bucket":
            buckets = random.choice([5, 10])
            mode = random.choice(["zscore", "rank", "mean"])
            res = self._apply_bucket(series, buckets, mode)
            return res, UnaryStep(
                name=f"{op}_{mode}", params={"buckets": buckets}
            )
        if op == "cs_zscore":
            res = _demean(series) / (_cs_apply(series, "std") + EPS)
            return res, UnaryStep(name=op, params={})
        if op == "cs_rank":
            res = series.groupby(level="pred_date").transform(_safe_rank)
            return res, UnaryStep(name=op, params={})
        if op == "vector_neutralize_a_on_rank":
            ranks = series.groupby(level="pred_date").transform(_safe_rank)
            res = self._cross_sectional_regression(series, ranks)["resid"]
            return res, UnaryStep(name=op, params={})
        if op == "vector_neutralize_rank_on_a":
            ranks = series.groupby(level="pred_date").transform(_safe_rank)
            res = self._cross_sectional_regression(ranks, series)["resid"]
            return res, UnaryStep(name=op, params={})
        if op == "cs_regression_resid":
            ranks = series.groupby(level="pred_date").transform(_safe_rank)
            res = self._cross_sectional_regression(series, ranks)["resid"]
            return res, UnaryStep(name=op, params={})
        if op == "ts_rank_scale":
            window = random.randint(3, 6)
            res = (
                series.groupby(level="code")
                .rolling(window=window, min_periods=1)
                .apply(lambda x: statistics.mean(_safe_rank(pd.Series(x))), raw=False)
                .droplevel(0)
            )
            return res, UnaryStep(name=op, params={"window": window})
        raise ValueError(op)

    # ------------------------------------------------------------------
    # binary operators
    # ------------------------------------------------------------------

    def apply_binary(
        self, lhs: pd.Series, rhs: pd.Series, op: str
    ) -> Tuple[pd.Series, BinaryStep]:
        lhs, rhs = _align_series_pair(lhs, rhs)
        if op == "rank_ratio":
            rl = lhs.groupby(level="pred_date").transform(_safe_rank)
            rr = rhs.groupby(level="pred_date").transform(_safe_rank)
            res = (rl - rr) / (rl + rr + EPS)
            return res, BinaryStep(name=op, params={})
        if op == "abs_rank_ratio":
            rl = lhs.groupby(level="pred_date").transform(_safe_rank)
            rr = rhs.groupby(level="pred_date").transform(_safe_rank)
            res = (rl - rr).abs() / (rl + rr + EPS)
            return res, BinaryStep(name=op, params={})
        if op == "ts_regression_pair":
            window = 5
            res = self._rolling_regression(lhs, rhs, window, random.choice(["slope", "intercept", "rmse"]))
            return res, BinaryStep(name=op, params={"window": window})
        if op == "ts_corr":
            res = self._rolling_corr(lhs, rhs, window=5)
            return res, BinaryStep(name=op, params={"window": 5})
        if op == "product":
            return lhs * rhs, BinaryStep(name=op, params={})
        if op == "factor_neutralize_pair":
            res = self._cross_sectional_regression(lhs, rhs)["resid"]
            return res, BinaryStep(name=op, params={})
        if op == "cs_regression_resid_pair":
            res = self._cross_sectional_regression(lhs, rhs)["resid"]
            return res, BinaryStep(name=op, params={})
        if op == "bucket_transform":
            buckets = random.choice([5, 10])
            mode = random.choice(["rank", "zscore", "mean"])
            grouped = rhs.groupby(level="pred_date").transform(
                lambda s: pd.qcut(s.rank(method="first"), buckets, labels=False, duplicates="drop")
            )
            res = (
                pd.DataFrame({"group": grouped, "lhs": lhs})
                .groupby(level="pred_date", group_keys=False)
                .apply(
                    lambda df: df.groupby("group")["lhs"].transform(
                        mode if mode != "mean" else "mean"
                    )
                )
            )
            res.index = lhs.index
            res = res.sort_index()
            return res, BinaryStep(name=f"{op}_{mode}", params={"buckets": buckets})
        if op == "centered_rank_product":
            rl = lhs.groupby(level="pred_date").transform(_safe_rank) - 0.5
            rr = rhs.groupby(level="pred_date").transform(_safe_rank) - 0.5
            return rl * rr, BinaryStep(name=op, params={})
        if op == "ts_zscore_product":
            zl = self.apply_unary(lhs, "ts_zscore")[0]
            zr = self.apply_unary(rhs, "ts_zscore")[0]
            return zl * zr, BinaryStep(name=op, params={"window": "2-5"})
        if op == "ts_cov":
            res = self._rolling_cov(lhs, rhs, window=5)
            return res, BinaryStep(name=op, params={"window": 5})
        if op == "rolling_rank_gap":
            res = self._rolling_rank_gap(lhs, rhs, window=5)
            return res, BinaryStep(name=op, params={"window": 5})
        raise ValueError(op)

    # ------------------------------------------------------------------
    # helper calculations
    # ------------------------------------------------------------------

    def _rolling_regression(
        self, y: pd.Series, x: pd.Series, window: int, which: str
    ) -> pd.Series:
        def _calc(block_y: np.ndarray, block_x: np.ndarray) -> float:
            x_ = block_x
            y_ = block_y
            if np.all(np.isnan(x_)) or np.all(np.isnan(y_)):
                return np.nan
            mask = ~np.isnan(x_) & ~np.isnan(y_)
            if mask.sum() < 2:
                return np.nan
            x_ = x_[mask]
            y_ = y_[mask]
            x_mean = x_.mean()
            y_mean = y_.mean()
            cov = ((x_ - x_mean) * (y_ - y_mean)).sum()
            var = ((x_ - x_mean) ** 2).sum()
            slope = cov / (var + EPS)
            intercept = y_mean - slope * x_mean
            resid = y_ - (slope * x_ + intercept)
            if which == "slope":
                return slope
            if which == "intercept":
                return intercept
            rmse = np.sqrt(np.mean(resid ** 2))
            return rmse

        return _rolling_pair_apply(y, x, window, _calc)

    def _rolling_autocorr(self, series: pd.Series, lag: int, window: int) -> pd.Series:
        shifted = series.groupby(level="code").shift(lag)
        return self._rolling_corr(series, shifted, window)

    def _rolling_corr(
        self, a: pd.Series, b: pd.Series, window: int
    ) -> pd.Series:
        def _corr(x: np.ndarray, y: np.ndarray) -> float:
            mask = ~np.isnan(x) & ~np.isnan(y)
            if mask.sum() < 2:
                return np.nan
            return np.corrcoef(x[mask], y[mask])[0, 1]

        return _rolling_pair_apply(a, b, window, _corr)

    def _rolling_cov(
        self, a: pd.Series, b: pd.Series, window: int
    ) -> pd.Series:
        def _cov(x: np.ndarray, y: np.ndarray) -> float:
            mask = ~np.isnan(x) & ~np.isnan(y)
            if mask.sum() < 2:
                return np.nan
            return np.cov(x[mask], y[mask])[0, 1]

        return _rolling_pair_apply(a, b, window, _cov)

    def _rolling_rank_gap(
        self, a: pd.Series, b: pd.Series, window: int
    ) -> pd.Series:
        def _gap(x: np.ndarray, y: np.ndarray) -> float:
            mask = ~np.isnan(x) & ~np.isnan(y)
            if mask.sum() == 0:
                return np.nan
            xr = pd.Series(x[mask]).rank().values
            yr = pd.Series(y[mask]).rank().values
            return np.sum(np.abs(np.sort(xr) - np.sort(yr)))

        return _rolling_pair_apply(a, b, window, _gap)

    def _cross_sectional_regression(
        self, y: pd.Series, x: pd.Series
    ) -> Dict[str, pd.Series]:
        def _fit(block: pd.DataFrame) -> pd.DataFrame:
            df = block.dropna()
            if df.shape[0] < 2:
                return pd.DataFrame(
                    {
                        "slope": np.nan,
                        "intercept": np.nan,
                        "resid": np.nan,
                    },
                    index=block.index,
                )
            x_ = df["x"]
            y_ = df["y"]
            slope, intercept = np.polyfit(x_, y_, 1)
            resid = y_ - (slope * x_ + intercept)
            out = pd.DataFrame(
                {
                    "slope": slope,
                    "intercept": intercept,
                    "resid": resid,
                },
                index=df.index,
            )
            return out.reindex(block.index)

        joined = pd.concat([y.rename("y"), x.rename("x")], axis=1)
        res = joined.groupby(level="pred_date", group_keys=False).apply(_fit)
        res.index = joined.index
        return {
            "slope": res["slope"],
            "intercept": res["intercept"],
            "resid": res["resid"],
        }

    def _apply_bucket(self, series: pd.Series, buckets: int, mode: str) -> pd.Series:
        def _transform(block: pd.Series) -> pd.Series:
            ranks = block.rank(method="first")
            labels = pd.qcut(
                ranks,
                q=min(buckets, block.notna().sum()),
                labels=False,
                duplicates="drop",
            )
            df = pd.DataFrame({"value": block, "label": labels})
            if mode == "zscore":
                agg = df.groupby("label")["value"].transform(lambda s: (s - s.mean()) / (s.std() + EPS))
            elif mode == "rank":
                agg = df.groupby("label")["value"].transform(lambda s: s.rank(pct=True))
            else:
                agg = df.groupby("label")["value"].transform("mean")
            return agg

        res = series.groupby(level="pred_date", group_keys=False).apply(_transform)
        res.index = series.index
        return res.sort_index()


# ---------------------------------------------------------------------------
# Distribution engine
# ---------------------------------------------------------------------------


class DistributionEngine:
    """Create the 6 (distribution x scaling) variants."""

    DISTRIBUTIONS = ("raw", "rank", "gaussian")
    SCALERS = ("norm1", "norm_mean")

    def __call__(self, series: pd.Series) -> Dict[str, pd.Series]:
        out: Dict[str, pd.Series] = {}
        for dist in self.DISTRIBUTIONS:
            if dist == "raw":
                transformed = series
            elif dist == "rank":
                transformed = series.groupby(level="pred_date").transform(_safe_rank)
            else:
                transformed = series.groupby(level="pred_date").transform(_gaussianize)
            demeaned = _demean(transformed)
            for scaler in self.SCALERS:
                if scaler == "norm1":
                    scaled = _normalize_long_short(demeaned)
                else:
                    scaled = _normalize_long_short_mean(demeaned)
                out[f"{dist}_{scaler}"] = scaled
        return out


# ---------------------------------------------------------------------------
# Metrics
# ---------------------------------------------------------------------------


@dataclass
class MetricResult:
    ic: float
    lw_ic_v2: float
    raw: Optional[Dict[str, object]] = None


class TouchstoneMetricBook:
    def __init__(self, state: str = "core", T: int = 20):
        self.state = state
        self.T = T

    def evaluate(self, series: pd.Series, label: str) -> MetricResult:
        factor_df = series.to_frame(name=label)
        try:
            res = analyze(factor_df, state=self.state, T=self.T)
        except Exception as exc:  # pragma: no cover - external service errors
            print(f"[TouchstoneMetricBook] analyze failed for {label}: {exc}")
            return MetricResult(ic=np.nan, lw_ic_v2=np.nan)

        ic_vals = np.asarray(res.get("ic", []), dtype="float64")
        lw_vals = np.asarray(res.get("lw_ic_v2", []), dtype="float64")

        ic = float(np.nanmean(ic_vals)) if ic_vals.size else float("nan")
        lw_ic = float(np.nanmean(lw_vals)) if lw_vals.size else float("nan")

        return MetricResult(ic=ic, lw_ic_v2=lw_ic, raw=res)


# ---------------------------------------------------------------------------
# Combo manager
# ---------------------------------------------------------------------------


@dataclass
class ComboEntry:
    name: str
    weight: float
    description: Dict[str, object]


class AlphaComboManager:
    def __init__(
        self,
        metric_book: TouchstoneMetricBook,
        log_path: str,
        combo_parquet: str,
        figure_path: str,
    ):
        self.metric_book = metric_book
        self.log_path = log_path
        self.combo_parquet = combo_parquet
        self.figure_path = figure_path
        self.entries: List[ComboEntry] = []
        self.combo_series: Optional[pd.Series] = None
        self.combo_metrics = MetricResult(ic=0.0, lw_ic_v2=0.0)
        self.best_combo_series: Optional[pd.Series] = None
        self.best_metric_sum = -np.inf
        self.combo_raw: Optional[Dict[str, object]] = None
        if os.path.exists(self.log_path):
            with open(self.log_path, "a", encoding="utf-8") as fp:
                fp.write("\n---- Restarting session ----\n")

    def attempt_add(
        self,
        candidate_name: str,
        variants: Dict[str, pd.Series],
        structure: AlphaStructure,
        base_metrics: Dict[str, MetricResult],
    ) -> None:
        structure_dict = self._structure_to_dict(structure)
        for variant_key, variant_series in variants.items():
            variant_series = variant_series.sort_index()
            metrics = base_metrics.get(variant_key)
            if metrics is None:
                metrics = self.metric_book.evaluate(
                    variant_series, label=f"{candidate_name}_{variant_key}"
                )
                base_metrics[variant_key] = metrics
            if pd.isna(metrics.ic) or pd.isna(metrics.lw_ic_v2):
                continue
            working_series = variant_series
            working_metrics = metrics
            flipped = False
            if working_metrics.ic < 0:
                working_series = (-variant_series).sort_index()
                working_metrics = self.metric_book.evaluate(
                    working_series, label=f"{candidate_name}_{variant_key}_flipped"
                )
                flipped = True
            if (
                pd.isna(working_metrics.ic)
                or pd.isna(working_metrics.lw_ic_v2)
                or working_metrics.ic < 0.004
            ):
                continue
            for weight in (2.0, 1.0, 0.5, 0.1):
                previous_metrics = self.combo_metrics
                combined_series = self._combine_series(working_series, weight)
                combined_metrics = self.metric_book.evaluate(
                    combined_series, label="combo"
                )
                if self._is_improvement(combined_metrics):
                    self.combo_series = combined_series
                    self.entries.append(
                        ComboEntry(
                            name=f"{candidate_name}:{variant_key}",
                            weight=weight if not flipped else -weight,
                            description=structure_dict,
                        )
                    )
                    self.combo_metrics = combined_metrics
                    self.combo_raw = combined_metrics.raw
                    self._log_success(
                        candidate_name,
                        variant_key,
                        weight if not flipped else -weight,
                        combined_metrics,
                        working_metrics,
                        flipped,
                        structure_dict,
                        previous_metrics,
                    )
                    self._persist_combo()
                    break

    def _combine_series(self, series: pd.Series, weight: float) -> pd.Series:
        series = series.sort_index() * weight
        if self.combo_series is None:
            return series
        combo = self.combo_series.sort_index()
        df = pd.concat(
            [combo.rename("combo"), series.rename("candidate")], axis=1
        )
        combined = df.sum(axis=1, min_count=1)
        combined.name = "alpha_combo"
        return combined

    def _is_improvement(self, metrics: MetricResult) -> bool:
        if pd.isna(metrics.ic) or pd.isna(metrics.lw_ic_v2):
            return False
        if metrics.ic < self.combo_metrics.ic - EPS:
            return False
        if metrics.lw_ic_v2 < self.combo_metrics.lw_ic_v2 - EPS:
            return False
        delta = (metrics.ic + metrics.lw_ic_v2) - (
            self.combo_metrics.ic + self.combo_metrics.lw_ic_v2
        )
        return delta > 5e-4

    def _log_success(
        self,
        candidate_name: str,
        variant_key: str,
        weight: float,
        new_metrics: MetricResult,
        variant_metrics: MetricResult,
        flipped: bool,
        structure: Dict[str, object],
        previous_metrics: MetricResult,
    ) -> None:
        os.makedirs(os.path.dirname(self.log_path) or ".", exist_ok=True)
        payload = {
            "candidate": candidate_name,
            "variant": variant_key,
            "weight": weight,
            "flipped": flipped,
            "variant_metrics": dataclasses.asdict(variant_metrics),
            "combo_metrics": dataclasses.asdict(new_metrics),
            "previous_combo_metrics": dataclasses.asdict(previous_metrics),
            "structure": structure,
        }
        with open(self.log_path, "a", encoding="utf-8") as fp:
            fp.write(json.dumps(payload, ensure_ascii=False) + "\n")
        print(
            (
                f"[combo] Added {candidate_name}:{variant_key} (weight {weight:+.2f})"
                f" | IC {previous_metrics.ic:.6f} -> {new_metrics.ic:.6f}"
                f" | LW_IC_V2 {previous_metrics.lw_ic_v2:.6f} -> {new_metrics.lw_ic_v2:.6f}"
            ),
            flush=True,
        )

    def _persist_combo(self) -> None:
        total = self.combo_metrics.ic + self.combo_metrics.lw_ic_v2
        if total <= self.best_metric_sum:
            return
        self.best_metric_sum = total
        self.best_combo_series = self.combo_series
        os.makedirs(os.path.dirname(self.combo_parquet) or ".", exist_ok=True)
        combo_df = self.combo_series.to_frame(name="alpha_combo")
        combo_df.to_parquet(self.combo_parquet)
        if self.combo_raw:
            _save_combo_plot(self.combo_raw, self.figure_path)

    def _structure_to_dict(self, structure: AlphaStructure) -> Dict[str, object]:
        return {
            "depth": structure.depth,
            "composites": [dataclasses.asdict(step) for step in structure.composite_steps],
            "unary": [dataclasses.asdict(step) for step in structure.unary_steps],
            "binary": [dataclasses.asdict(step) for step in structure.binary_steps],
        }


# ---------------------------------------------------------------------------
# plotting helpers
# ---------------------------------------------------------------------------


def _save_combo_plot(res_dict: Dict[str, object], figure_path: str) -> None:
    dates = safe_get_dates(res_dict)
    ic_vals = np.asarray(res_dict.get("ic", []), dtype="float64")
    lw_vals = np.asarray(res_dict.get("lw_ic_v2", []), dtype="float64")

    if not dates or ic_vals.size == 0 or lw_vals.size == 0:
        return

    os.makedirs(os.path.dirname(figure_path) or ".", exist_ok=True)

    fig, (ax0, ax1) = plt.subplots(2, 1, figsize=(12, 10), sharex=True)
    ax0.plot(dates, ic_vals, label="IC")
    ax0.plot(dates, lw_vals, label="LW_IC_V2")
    ax0.legend()
    ax0.set_title("Daily metrics for best combo")

    ax1.plot(dates, np.nancumsum(ic_vals), label="CumSum IC")
    ax1.plot(dates, np.nancumsum(lw_vals), label="CumSum LW_IC_V2")
    ax1.legend()
    ax1.set_title("Cumulative metrics")

    fig.tight_layout()
    fig.savefig(figure_path)
    plt.close(fig)


# ---------------------------------------------------------------------------
# Alpha builder
# ---------------------------------------------------------------------------


class AlphaBuilder:
    def __init__(self, operator_lib: OperatorLibrary):
        self.oplib = operator_lib

    def build(self) -> Tuple[pd.Series, AlphaStructure]:
        depth = random.randint(1, 4)
        unary_used = False
        current: Optional[pd.Series] = None
        composite_steps: List[CompositeStep] = []
        unary_steps: List[UnaryStep] = []
        binary_steps: List[BinaryStep] = []

        for _ in range(depth):
            comp_series, comp_step = self.oplib.random_composite()
            composite_steps.append(comp_step)

            if current is None:
                current = comp_series
                continue

            use_unary = (not unary_used) and (random.random() < 0.3)
            current, comp_series = _align_series_pair(current, comp_series)
            if use_unary:
                op = random.choice(self.oplib.UNARY_OPS)
                current, unary_step = self.oplib.apply_unary(current, op)
                unary_steps.append(unary_step)
                unary_used = True
                current = current + comp_series
            else:
                op = random.choice(self.oplib.BINARY_OPS)
                current, binary_step = self.oplib.apply_binary(current, comp_series, op)
                binary_steps.append(binary_step)

        if current is None:
            raise RuntimeError("Failed to build alpha – no composites generated.")

        current = current.sort_index()
        structure = AlphaStructure(
            depth=depth,
            composite_steps=composite_steps,
            unary_steps=unary_steps,
            binary_steps=binary_steps,
        )
        return current, structure


# ---------------------------------------------------------------------------
# High level driver
# ---------------------------------------------------------------------------


def run_search(
    df: pd.DataFrame,
    iterations: int,
    seed: Optional[int],
    log_path: str,
    combo_parquet: str,
    figure_path: str,
    touchstone_state: str,
    touchstone_T: int,
):
    if seed is not None:
        random.seed(seed)
        np.random.seed(seed)

    workspace = AlphaWorkspace(df)
    operator_lib = OperatorLibrary(workspace)
    builder = AlphaBuilder(operator_lib)
    metric_book = TouchstoneMetricBook(state=touchstone_state, T=touchstone_T)
    combo_manager = AlphaComboManager(
        metric_book,
        log_path=log_path,
        combo_parquet=combo_parquet,
        figure_path=figure_path,
    )
    distribution_engine = DistributionEngine()

    for iteration in range(1, iterations + 1):
        series, structure = builder.build()
        candidate_name = f"alpha_{iteration:05d}"
        variants = distribution_engine(series)
        base_metrics: Dict[str, MetricResult] = {}
        combo_manager.attempt_add(candidate_name, variants, structure, base_metrics)


def _parse_args(argv: Optional[Sequence[str]] = None) -> argparse.Namespace:
    parser = argparse.ArgumentParser(
        description="Randomly expand dataframe features into a combo alpha",
    )
    parser.add_argument(
        "--parquet-path",
        default=DEFAULT_PARQUET_PATH,
        help=(
            "Path to the parquet file produced by gen_all_date_feature.py. "
            "Defaults to the shared demo file if not provided."
        ),
    )
    parser.add_argument(
        "--iterations",
        type=int,
        default=200,
        help="Number of stochastic iterations to run",
    )
    parser.add_argument(
        "--seed",
        type=int,
        default=None,
        help="Random seed for reproducibility",
    )
    parser.add_argument(
        "--log-path",
        default="combo_progress.txt",
        help="TXT file used to log successful alpha additions",
    )
    parser.add_argument(
        "--combo-parquet",
        default="best_combo.parquet",
        help="Parquet path that stores the strongest combo so far",
    )
    parser.add_argument(
        "--figure-path",
        default="figures/best_combo.png",
        help="PNG figure storing the metrics of the strongest combo",
    )
    parser.add_argument(
        "--touchstone-state",
        default="core",
        help="State argument forwarded to Touchstone analyze()",
    )
    parser.add_argument(
        "--touchstone-T",
        type=int,
        default=20,
        help="Window length forwarded to Touchstone analyze()",
    )
    return parser.parse_args(argv)


def main(argv: Optional[Sequence[str]] = None) -> None:
    args = _parse_args(argv)
    parquet_path = os.path.expanduser(args.parquet_path)
    if not os.path.exists(parquet_path):
        raise FileNotFoundError(
            f"Could not locate parquet file at '{parquet_path}'. "
            "Pass --parquet-path with the correct location."
        )
    df = pd.read_parquet(parquet_path)
    run_search(
        df=df,
        iterations=args.iterations,
        seed=args.seed,
        log_path=args.log_path,
        combo_parquet=args.combo_parquet,
        figure_path=args.figure_path,
        touchstone_state=args.touchstone_state,
        touchstone_T=args.touchstone_T,
    )


if __name__ == "__main__":
    main()

